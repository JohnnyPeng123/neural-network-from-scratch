{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import h5py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = h5py.File('train_128.h5', 'r')\n",
    "list(f1.keys())\n",
    "X1 = f1['data']\n",
    "X_full= np.array(X1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = h5py.File('train_label.h5', 'r')\n",
    "list(f1.keys())\n",
    "Y1 = f1['label']\n",
    "Y_full= np.array(Y1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    #Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T \n",
    "    return OHX\n",
    "\n",
    "def normalize(X):\n",
    "    mu_train = np.mean(X,axis=0,keepdims=True)\n",
    "    var_train = np.var(X,axis=0,keepdims=True)\n",
    "    X_norm = (X-mu_train) / np.sqrt(var_train)\n",
    "    \n",
    "    return X_norm, mu_train, var_train\n",
    "\n",
    "def normalize_test(X_norm, mu_train, var_train):\n",
    "    X_norm = (X_norm-mu_train) / np.sqrt(var_train)\n",
    "    \n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50000)\n",
      "(10, 50000)\n",
      "(128, 10000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_X = X_full[0:50000]\n",
    "train_X, mu_train, var_train = normalize(train_X)\n",
    "train_X = train_X.T\n",
    "\n",
    "train_Y = Y_full[0:50000]\n",
    "train_Y = oneHotIt(train_Y).T\n",
    "\n",
    "dev_X = X_full[50000:]\n",
    "dev_X = normalize_test(dev_X,mu_train,var_train)\n",
    "dev_X = dev_X.T\n",
    "\n",
    "dev_Y = Y_full[50000:]\n",
    "dev_Y = oneHotIt(dev_Y).T\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(dev_X.shape)\n",
    "print(dev_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = a3.T\n",
    "    labels = Y.T\n",
    "    \n",
    "    cost = -np.sum((np.log(np.sum(labels*logits,axis=1, keepdims=True))))/logits.shape[0]\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = softmax(z3.T).T\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = a3 - Y\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((10,m), dtype = np.int)\n",
    "    accuracy = 0 \n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for j in range(0,10):\n",
    "        for i in range(0, a3.shape[1]):\n",
    "            if a3[j,i] == np.max(a3[:,i]):\n",
    "                p[j,i] = 1\n",
    "            else:\n",
    "                p[j,i] = 0\n",
    "    \n",
    "    for i in range(0,a3.shape[1]):\n",
    "        accuracy += (p[:,i] == y[:,i]).all()\n",
    "    accuracy = accuracy / a3.shape[1]\n",
    "    print(\"Accuracy: \"  + str(accuracy))\n",
    "    \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + W3*lambd/m\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + W2*lambd/m\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + W1*lambd/m\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (20, 2)\n",
    "                    b1 -- bias vector of shape (20, 1)\n",
    "                    W2 -- weight matrix of shape (3, 20)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
    "    cache -- tuple, information stored for computing the backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    D1 = np.random.rand(A1.shape[0],A1.shape[1])      # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob)                             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A1 = A1*D1                                        # Step 3: shut down some neurons of A1\n",
    "    A1 = A1/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    D2 = np.random.rand(A2.shape[0],A2.shape[1])       # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    D2 = (D2 < keep_prob)                              # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A2 = A2*D2                                         # Step 3: shut down some neurons of A2\n",
    "    A2 = A2/keep_prob                                  # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = softmax(Z3.T).T\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout_L2(X, Y, cache, keep_prob, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + W3*lambd/m\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    dA2 = dA2*D2                                    # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2/keep_prob                             # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + W2*lambd/m\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "\n",
    "    dA1 = dA1*D1                                   # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1/keep_prob                            # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + W1*lambd/m\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    dA2 = dA2*D2                                     # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2/keep_prob                              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1 = dA1*D1                                     # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1/keep_prob                              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_mean_var(x, eps):\n",
    "\n",
    "  N, D = x.shape\n",
    "\n",
    "  #step1: calculate mean\n",
    "  mu = 1./N * np.sum(x, axis = 0)\n",
    "\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "  xmu = x - mu\n",
    "\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "  sq = xmu ** 2\n",
    "\n",
    "  #step4: calculate variance\n",
    "  var = 1./N * np.sum(sq, axis = 0)\n",
    "\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "  sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "  #step6: invert sqrtwar\n",
    "  ivar = 1./sqrtvar\n",
    "\n",
    "  return mu,ivar,sqrtvar,var,eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W' + str(l+1)] - learning_rate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b' + str(l+1)] - learning_rate*grads['db' + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        \n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "   \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- python dictionary containing your updated velocities\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "    \n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)]+(1-beta)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)]+(1-beta)*grads['db' + str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v[\"db\" + str(l+1)]\n",
    "     \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads[\"db\" + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**(t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**(t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*(grads[\"dW\" + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*(grads[\"db\" + str(l+1)]**2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**(t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**(t))\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v_corrected[\"dW\" + str(l+1)]/((s_corrected[\"dW\" + str(l+1)])**(1/2)+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v_corrected[\"db\" + str(l+1)]/((s_corrected[\"db\" + str(l+1)])**(1/2)+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, regularizer = \"No\",keep_prob = 0.85, lambd = 0.7, learning_rate_init = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        learning_rate = learning_rate_init * (1/(1+0.01*i))\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            if regularizer == \"DropOut\":\n",
    "                a3, caches = forward_propagation_with_dropout(minibatch_X, parameters,keep_prob)\n",
    "            elif regularizer == \"DropOut_L2\":\n",
    "                a3, caches = forward_propagation_with_dropout(minibatch_X, parameters,keep_prob)\n",
    "            else:\n",
    "                a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            if regularizer == \"No\":\n",
    "                grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "            elif regularizer == \"L2\":\n",
    "                grads = backward_propagation_with_regularization(minibatch_X, minibatch_Y, caches,lambd)\n",
    "            elif regularizer == \"DropOut\":\n",
    "                grads = backward_propagation_with_dropout(minibatch_X, minibatch_Y, caches, keep_prob)\n",
    "            elif regularizer == \"DropOut_L2\":\n",
    "                grads = backward_propagation_with_dropout_L2(minibatch_X, minibatch_Y, caches, keep_prob, lambd)\n",
    "                \n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 10)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.168472\n",
      "Cost after epoch 10: 0.603711\n",
      "Cost after epoch 20: 0.529841\n",
      "Cost after epoch 30: 0.458008\n",
      "Cost after epoch 40: 0.461499\n",
      "Cost after epoch 50: 0.440778\n",
      "Cost after epoch 60: 0.428472\n",
      "Cost after epoch 70: 0.457020\n",
      "Cost after epoch 80: 0.431228\n",
      "Cost after epoch 90: 0.391213\n",
      "Cost after epoch 100: 0.391579\n",
      "Cost after epoch 110: 0.437468\n",
      "Cost after epoch 120: 0.431644\n",
      "Cost after epoch 130: 0.459896\n",
      "Cost after epoch 140: 0.475018\n",
      "Cost after epoch 150: 0.391730\n",
      "Cost after epoch 160: 0.432670\n",
      "Cost after epoch 170: 0.409530\n",
      "Cost after epoch 180: 0.418987\n",
      "Cost after epoch 190: 0.384007\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8911272727272728\n",
      "Accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "    layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "    parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,regularizer=\"DropOut_L2\",keep_prob=0.9, lambd=0.7)\n",
    "\n",
    "    # Predict\n",
    "    predict(train_X, train_Y, parameters)\n",
    "    predict(dev_X, dev_Y, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
