{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import h5py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = h5py.File('train_128.h5', 'r')\n",
    "list(f1.keys())\n",
    "X1 = f1['data']\n",
    "X_full= np.array(X1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = h5py.File('train_label.h5', 'r')\n",
    "list(f1.keys())\n",
    "Y1 = f1['label']\n",
    "Y_full= np.array(Y1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    #Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "def normalize(X):\n",
    "    mu_train = np.mean(X,axis=0,keepdims=True)\n",
    "    var_train = np.var(X,axis=0,keepdims=True)\n",
    "    X_norm = (X-mu_train) / np.sqrt(var_train)\n",
    "    \n",
    "    return X_norm, mu_train, var_train\n",
    "\n",
    "def normalize_test(X_norm, mu_train, var_train):\n",
    "    X_norm = (X_norm-mu_train) / np.sqrt(var_train)\n",
    "    \n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50000)\n",
      "(10, 50000)\n",
      "(128, 10000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_X = X_full[0:50000]\n",
    "train_X, mu_train, var_train = normalize(train_X)\n",
    "train_X = train_X.T\n",
    "\n",
    "train_Y = Y_full[0:50000]\n",
    "train_Y = oneHotIt(train_Y).T\n",
    "\n",
    "dev_X = X_full[50000:]\n",
    "dev_X = normalize_test(dev_X,mu_train,var_train)\n",
    "dev_X = dev_X.T\n",
    "\n",
    "dev_Y = Y_full[50000:]\n",
    "dev_Y = oneHotIt(dev_Y).T\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(dev_X.shape)\n",
    "print(dev_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = a3.T\n",
    "    labels = Y.T\n",
    "    \n",
    "    cost = -np.sum((np.log(np.sum(labels*logits,axis=1, keepdims=True))))/logits.shape[0]\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = softmax(z3.T).T\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = a3 - Y\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((10,m), dtype = np.int)\n",
    "    accuracy = 0 \n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for j in range(0,10):\n",
    "        for i in range(0, a3.shape[1]):\n",
    "            if a3[j,i] == np.max(a3[:,i]):\n",
    "                p[j,i] = 1\n",
    "            else:\n",
    "                p[j,i] = 0\n",
    "    \n",
    "    for i in range(0,a3.shape[1]):\n",
    "        accuracy += (p[:,i] == y[:,i]).all()\n",
    "    accuracy = accuracy / a3.shape[1]\n",
    "    print(\"Accuracy: \"  + str(accuracy))\n",
    "    \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + W3*lambd/m\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + W2*lambd/m\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + W1*lambd/m\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (20, 2)\n",
    "                    b1 -- bias vector of shape (20, 1)\n",
    "                    W2 -- weight matrix of shape (3, 20)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
    "    cache -- tuple, information stored for computing the backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    D1 = np.random.rand(A1.shape[0],A1.shape[1])      # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob)                             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A1 = A1*D1                                        # Step 3: shut down some neurons of A1\n",
    "    A1 = A1/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    D2 = np.random.rand(A2.shape[0],A2.shape[1])       # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    D2 = (D2 < keep_prob)                              # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A2 = A2*D2                                         # Step 3: shut down some neurons of A2\n",
    "    A2 = A2/keep_prob                                  # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = softmax(Z3.T).T\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout_L2(X, Y, cache, keep_prob, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + W3*lambd/m\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    dA2 = dA2*D2                                    # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2/keep_prob                             # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + W2*lambd/m\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "\n",
    "    dA1 = dA1*D1                                   # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1/keep_prob                            # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + W1*lambd/m\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    dA2 = dA2*D2                                     # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2/keep_prob                              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1 = dA1*D1                                     # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1/keep_prob                              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_mean_var(x, eps):\n",
    "\n",
    "  N, D = x.shape\n",
    "\n",
    "  #step1: calculate mean\n",
    "  mu = 1./N * np.sum(x, axis = 0)\n",
    "\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "  xmu = x - mu\n",
    "\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "  sq = xmu ** 2\n",
    "\n",
    "  #step4: calculate variance\n",
    "  var = 1./N * np.sum(sq, axis = 0)\n",
    "\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "  sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "  #step6: invert sqrtwar\n",
    "  ivar = 1./sqrtvar\n",
    "\n",
    "  return mu,ivar,sqrtvar,var,eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W' + str(l+1)] - learning_rate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b' + str(l+1)] - learning_rate*grads['db' + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        \n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "   \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- python dictionary containing your updated velocities\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "    \n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)]+(1-beta)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)]+(1-beta)*grads['db' + str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v[\"db\" + str(l+1)]\n",
    "     \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads[\"db\" + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**(t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**(t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*(grads[\"dW\" + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*(grads[\"db\" + str(l+1)]**2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**(t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**(t))\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v_corrected[\"dW\" + str(l+1)]/((s_corrected[\"dW\" + str(l+1)])**(1/2)+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v_corrected[\"db\" + str(l+1)]/((s_corrected[\"db\" + str(l+1)])**(1/2)+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, regularizer = \"No\",keep_prob = 0.85, lambd = 0.7, learning_rate_init = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        learning_rate = learning_rate_init \n",
    "        #learning_rate = learning_rate_init * (1/(1+0.01*i))\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            if regularizer == \"DropOut\":\n",
    "                a3, caches = forward_propagation_with_dropout(minibatch_X, parameters,keep_prob)\n",
    "            elif regularizer == \"DropOut_L2\":\n",
    "                a3, caches = forward_propagation_with_dropout(minibatch_X, parameters,keep_prob)\n",
    "            else:\n",
    "                a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            if regularizer == \"No\":\n",
    "                grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "            elif regularizer == \"L2\":\n",
    "                grads = backward_propagation_with_regularization(minibatch_X, minibatch_Y, caches,lambd)\n",
    "            elif regularizer == \"DropOut\":\n",
    "                grads = backward_propagation_with_dropout(minibatch_X, minibatch_Y, caches, keep_prob)\n",
    "            elif regularizer == \"DropOut_L2\":\n",
    "                grads = backward_propagation_with_dropout_L2(minibatch_X, minibatch_Y, caches, keep_prob, lambd)\n",
    "                \n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 10)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_prob: 0.3\n",
      "lambd: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-85fdbbaf54c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lambd: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"DropOut_L2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-a0fb4d0a7e8a>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, layers_dims, optimizer, regularizer, keep_prob, lambd, learning_rate_init, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mminibatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_mini_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#learning_rate = learning_rate_init * (1/(1+0.01*i))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-aaee4a29db63>\u001b[0m in \u001b[0;36mrandom_mini_batches\u001b[1;34m(X, Y, mini_batch_size, seed)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Step 1: Shuffle (X, Y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mshuffled_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mshuffled_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train 3-layer model - Adam\n",
    "for i in (0.3, 0.5, 0.7, 1):\n",
    "    for j in (0, 0.3, 0.5, 0.7, 1):\n",
    "        print(\"keep_prob: \" + str(i))\n",
    "        print(\"lambd: \" + str(j))\n",
    "        layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "        parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,regularizer=\"DropOut_L2\",keep_prob=i, lambd=j,print_cost = False)\n",
    "\n",
    "        # Predict\n",
    "        predict(train_X, train_Y, parameters)\n",
    "        predict(dev_X, dev_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.420343\n",
      "Cost after epoch 10: 1.037564\n",
      "Cost after epoch 20: 0.801640\n",
      "Cost after epoch 30: 0.724240\n",
      "Cost after epoch 40: 0.701449\n",
      "Cost after epoch 50: 0.657222\n",
      "Cost after epoch 60: 0.660748\n",
      "Cost after epoch 70: 0.627238\n",
      "Cost after epoch 80: 0.697139\n",
      "Cost after epoch 90: 0.666649\n",
      "Cost after epoch 100: 0.613812\n",
      "Cost after epoch 110: 0.644416\n",
      "Cost after epoch 120: 0.630746\n",
      "Cost after epoch 130: 0.683712\n",
      "Cost after epoch 140: 0.699986\n",
      "Cost after epoch 150: 0.620905\n",
      "Cost after epoch 160: 0.688686\n",
      "Cost after epoch 170: 0.611154\n",
      "Cost after epoch 180: 0.628291\n",
      "Cost after epoch 190: 0.645841\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam + Dropout with Weight Decay\n",
      "Training Accuarcy: \n",
      "Accuracy: 0.86948\n",
      "Test Accuarcy: \n",
      "Accuracy: 0.8584\n",
      "\n",
      "Cost after epoch 0: 2.250546\n",
      "Cost after epoch 10: 2.311876\n",
      "Cost after epoch 20: 2.270189\n",
      "Cost after epoch 30: 2.294602\n",
      "Cost after epoch 40: 2.280227\n",
      "Cost after epoch 50: 2.355163\n",
      "Cost after epoch 60: 2.275923\n",
      "Cost after epoch 70: 2.281341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6395e15db5dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# train 3-layer model - Minibatch SGD with Momentum + Dropout_L2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"momentum\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00225\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DropOut_L2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-cc58c92e8f73>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, layers_dims, optimizer, regularizer, keep_prob, lambd, learning_rate_init, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation_with_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DropOut_L2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation_with_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-597005e569a2>\u001b[0m in \u001b[0;36mforward_propagation_with_dropout\u001b[1;34m(X, parameters, keep_prob)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mD1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# Step 1: initialize matrix D1 = np.random.rand(..., ...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train 3-layer model - Adam + Dropout_L2\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,learning_rate_init = 0.0007,regularizer = \"DropOut_L2\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Adam + Dropout with Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - Minibatch SGD with Momentum + Dropout_L2\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\",num_epochs = 200,mini_batch_size = 1,learning_rate_init = 0.00007,regularizer = \"DropOut_L2\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Minibatch GD with Momentum + Dropout with Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - GD + Dropout_L2\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\",num_epochs = 200,mini_batch_size = 50000,learning_rate_init = 1,regularizer = \"DropOut_L2\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"GD + Dropout with Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.367857\n",
      "Cost after epoch 10: 0.719520\n",
      "Cost after epoch 20: 0.557748\n",
      "Cost after epoch 30: 0.438340\n",
      "Cost after epoch 40: 0.373082\n",
      "Cost after epoch 50: 0.352060\n",
      "Cost after epoch 60: 0.367889\n",
      "Cost after epoch 70: 0.321277\n",
      "Cost after epoch 80: 0.320698\n",
      "Cost after epoch 90: 0.270974\n",
      "Cost after epoch 100: 0.296253\n",
      "Cost after epoch 110: 0.336461\n",
      "Cost after epoch 120: 0.332615\n",
      "Cost after epoch 130: 0.329769\n",
      "Cost after epoch 140: 0.327614\n",
      "Cost after epoch 150: 0.253603\n",
      "Cost after epoch 160: 0.318444\n",
      "Cost after epoch 170: 0.271955\n",
      "Cost after epoch 180: 0.287924\n",
      "Cost after epoch 190: 0.280376\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam + Dropout with Weight Decay\n",
      "Training Accuarcy: \n",
      "Accuracy: 0.92966\n",
      "Test Accuarcy: \n",
      "Accuracy: 0.8896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train 3-layer model - Adam + Dropout_L2\n",
    "layers_dims = [train_X.shape[0], 200, 100, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,learning_rate_init = 0.0001,regularizer = \"DropOut_L2\",keep_prob=0.8, lambd=1.75)\n",
    "\n",
    "# Predict\n",
    "print(\"Adam + Dropout with Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model - Adam + Dropout\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,learning_rate_init = 0.0007,regularizer = \"DropOut\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Adam + Dropout\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - Minibatch SGD with Momentum + Dropout\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\",num_epochs = 200,mini_batch_size = 1,learning_rate_init = 0.00007,regularizer = \"DropOut\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Minibatch GD with Momentum + Dropout\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - GD + Dropout\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\",num_epochs = 200,mini_batch_size = 50000,learning_rate_init = 1,regularizer = \"DropOut\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"GD + Dropout\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model - Adam + L2\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,learning_rate_init = 0.0007,regularizer = \"L2\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Adam +  Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - Minibatch SGD with Momentum + L2\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\",num_epochs = 200,mini_batch_size = 1,learning_rate_init = 0.00007,regularizer = \"L2\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Minibatch GD with Momentum +  Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - GD + L2\n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\",num_epochs = 200,mini_batch_size = 50000,learning_rate_init = 1,regularizer = \"L2\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"GD + Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.117079\n",
      "Cost after epoch 10: 0.430267\n",
      "Cost after epoch 20: 0.403626\n",
      "Cost after epoch 30: 0.308258\n",
      "Cost after epoch 40: 0.311558\n",
      "Cost after epoch 50: 0.290981\n",
      "Cost after epoch 60: 0.306106\n",
      "Cost after epoch 70: 0.265307\n",
      "Cost after epoch 80: 0.295748\n",
      "Cost after epoch 90: 0.239443\n",
      "Cost after epoch 100: 0.269384\n",
      "Cost after epoch 110: 0.306680\n",
      "Cost after epoch 120: 0.284993\n",
      "Cost after epoch 130: 0.294481\n",
      "Cost after epoch 140: 0.309517\n",
      "Cost after epoch 150: 0.240741\n",
      "Cost after epoch 160: 0.299013\n",
      "Cost after epoch 170: 0.260665\n",
      "Cost after epoch 180: 0.270728\n",
      "Cost after epoch 190: 0.277243\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XucnHV99//Xe4/JZjbH2SSQhBw4KmeIgIKKtXKwFlSQQj3gAWmt9L5t+7t/1bu9hR/W/uzB2lpURBuBVhEV0WixiLVyEEGWNEACQkISyCYk2ewm2SSbZE+f+4/r2mTYzEwmu5mdTeb9fDyux8x8r+8189nZ3fnM9/pe1+dSRGBmZnYgNZUOwMzMDg9OGGZmVhInDDMzK4kThpmZlcQJw8zMSuKEYWZmJXHCsCOWpJ9IurbScZgdKZww7JCTtEbSb1c6joi4NCLuqHQcAJJ+Iem6CrzuVEn3Stop6SVJv1+kryT9jaSOdPlbScpZf4akJyV1p7dnlLKtpDdK2jFkCUlXlPent0PNCcMOS5LqKh3DoLEUSx5fAnqAGcB7ga9IOrlA3+uBdwKnA6cB7wD+AEBSA/BD4N+AKcAdwA/T9qLbRsTDEZEZXNJ1O4D/OLQ/qpWbE4aNKknvkLRU0lZJj0o6LWfdJyW9KGm7pGclvStn3Qcl/VLSFyR1AjelbY9I+ntJWyStlnRpzjZ7v9WX0He+pIfS1/6ZpC9J+rcCP8OFktok/bmkDcA3JE2R9GNJ7enz/1jS7LT/Z4E3Arek365vSdtPkvSApE5Jz0u66hC/1xOAK4D/ExE7IuIRYDHw/gKbXAt8PiLaImId8Hngg+m6C4E64B8jYk9EfBEQ8FslbJvvdb4XETtH8vPZ6HPCsFEj6SxgEck3z2nAV4HFkhrTLi+SfLBOAv4/4N8kHZXzFOcCq4DpwGdz2p4HssDfAv+SuxtliGJ9vwX8Oo3rJgp/qA6aCUwF5pJ8u64BvpE+PgbYBdwCEBF/ATwM3JB+y74h/TB/IH3d6cA1wJcLffuX9OU0yeZbni4Q4wlAf0S8kNP2FFBohHFyuj5f35OBp+PVtYSeHrK+0La5P0cTcCXJCMUOM04YNpo+Cnw1Ih6PiP50fmEPcB5ARHw3ItZHxEBE3A2sAM7J2X59RPxzRPRFxK607aWI+FpE9JN8CB1Fsvsln7x9JR0DvA74dET05HwTL2YAuDH9tr0rIjoi4p6I6I6I7SQJ7c1Ftn8HsCYivpH+PEuAe0g+TPcTEX8UEZMLLKfl2wbIANuGtG0Dmkvsvw3IpEn1QM9VbNtcVwCbgQcLxGBj2Fje92pHnrnAtZL+OKetATgaQNIHgD8F5qXrMiSjgUFr8zznhsE7EdGdfj5lCrx+ob5ZoDMiuoe81pwiP0t7ROwefJB+c/4CcAnJPn6AZkm1aYIaai5wrqStOW11wL8Wec2DtQOYOKRtIrC9xP4TgR0REZIO9FwFtx2yzbXAnXna7TDgEYaNprXAZ4d8O26KiLskzQW+BtwATIuIycAykv3kg8r1IfMKMDX90B9ULFnki+XPgBOBcyNiIvCmtF0F+q8FHhzyXmQi4mP5XkzSrXmONBpclheI8QWgTtLxOW2nA4X6L0/X5+u7HDhtyIjhtCHrC207+DPMIZkLubPA69sY54Rh5VIvaVzOUkeSEP5Q0rnpYZgTJP2OpGZgAsmHajuApA8Bp4xGoBHxEtBKMpHeIOn1wO8e5NM0k8xbbJU0FbhxyPqNwIKcxz8GTpD0fkn16fI6Sa8pEOMf5h5pNGTJOyeRTip/H7g5fa/PBy6n8CjmTuBPJc2SdDRJErw9XfcLoB/4H5IaJd2Qtv+8hG0HvR94NCJeLPD6NsY5YVi53EfyATq43BQRrSTzGLcAW4CVpEfSRMSzJEfW/Irkw/VU4JejGO97gdcDHcBfAXeTzK+U6h+B8ST75x9j/0NG/wm4Mj2C6ovpPMdFwNXAepLdZX8DNHJo/VEa1ybgLuBjEbEc9p0fkdP3q8CPgGdIRnf/nrYRET0kh81+ANgKfBh4Z9pedNscH8CT3Yc1eVei2f4k3Q38JiKGjhTMqpZHGGZAujvoWEk1ki4h2XXzg0rHZTaW+Cgps8RMkv3904A2kl03/13ZkMzGFu+SMjOzkniXlJmZleSI2iWVzWZj3rx5lQ7DzOyw8eSTT26OiJZS+h5RCWPevHm0trZWOgwzs8OGpJdK7etdUmZmVhInDDMzK4kThpmZlcQJw8zMSuKEYWZmJXHCMDOzkjhhmJlZSao+YUQE//yfK3jwhfZKh2JmNqZVfcKQxG0Pr+K/frOp0qGYmY1pVZ8wAFoyjbTvOJhr5ZiZVR8nDCCbaWTzdicMM7NinDCAbHMDmz3CMDMrygmDdISxo+fAHc3MqpgTBknC2Larl56+gUqHYmY2ZjlhkCQMgI6d3i1lZlaIEwaQzTQAsHm7d0uZmRXihAFkm5MRhie+zcwKK1vCkDRH0n9Jek7Sckn/M08fSfqipJWSnpZ0Vs66ayWtSJdryxUnJOdhALT70Fozs4LKeYnWPuDPImKJpGbgSUkPRMSzOX0uBY5Pl3OBrwDnSpoK3AgsBCLddnFEbClHoC3pCMMn75mZFVa2EUZEvBIRS9L724HngFlDul0O3BmJx4DJko4CLgYeiIjONEk8AFxSrljH1deSaazzLikzsyJGZQ5D0jzgTODxIatmAWtzHrelbYXayyabafC5GGZmRZQ9YUjKAPcAn4iIrqGr82wSRdrzPf/1kloltba3D7/irMuDmJkVV9aEIameJFl8MyK+n6dLGzAn5/FsYH2R9v1ExG0RsTAiFra0tAw71uRsbycMM7NCynmUlIB/AZ6LiH8o0G0x8IH0aKnzgG0R8QpwP3CRpCmSpgAXpW1l43pSZmbFlfMoqfOB9wPPSFqatv1v4BiAiLgVuA94O7AS6AY+lK7rlPQZ4Il0u5sjorOMsZLNNLKlu5fe/gHqa316ipnZUGVLGBHxCPnnInL7BPDxAusWAYvKEFpeg+VBOnf2MGPiuNF6WTOzw4a/SqeyPnnPzKwoJ4xUS3NaT8rzGGZmeTlhpFoyyW4ojzDMzPJzwkhl944wfPKemVk+ThippoY6mhpqvUvKzKwAJ4wcPnnPzKwwJ4wcST0pJwwzs3ycMHIk9aQ8h2Fmlo8TRo5ss3dJmZkV4oSRI5tppLO7h77+gUqHYmY25jhh5GjJNBABnd3eLWVmNpQTRo7B8iCexzAz258TRo5sem1vz2OYme3PCSNHiwsQmpkV5ISRwyMMM7PCnDByTGioZVx9jROGmVkeThg5JKXlQTzpbWY2lBPGEK4nZWaWX9kShqRFkjZJWlZg/f+StDRdlknqlzQ1XbdG0jPputZyxZhPNtPoSW8zszzKOcK4Hbik0MqI+LuIOCMizgA+BTwYEZ05Xd6Srl9Yxhj309Lc4F1SZmZ5lC1hRMRDQOcBOyauAe4qVywHI5tppHPnHvoHotKhmJmNKRWfw5DURDISuSenOYCfSnpS0vUH2P56Sa2SWtvb20ccTzbTyEDAFpcHMTN7lYonDOB3gV8O2R11fkScBVwKfFzSmwptHBG3RcTCiFjY0tIy4mD2lgfxxLeZ2auMhYRxNUN2R0XE+vR2E3AvcM5oBZPNpNf2dj0pM7NXqWjCkDQJeDPww5y2CZKaB+8DFwF5j7Qqh5b0bO/2HbtH6yXNzA4LdeV6Ykl3ARcCWUltwI1APUBE3Jp2exfw04jYmbPpDOBeSYPxfSsi/qNccQ61tzyIRxhmZq9StoQREdeU0Od2ksNvc9tWAaeXJ6oDa26so6HO5UHMzIYaC3MYY4okWjKNtDthmJm9ihNGHtmMT94zMxvKCSOPbKaRzS4PYmb2Kk4YebgAoZnZ/pww8sg2N9Cxs4cBlwcxM9vLCSOPbKaR/oFg667eSodiZjZmOGHk4fIgZmb7c8LIYzBh+LoYZmb7OGHkMVgexCMMM7N9nDDyaPEIw8xsP04YeUwcX0dDbY1P3jMzy+GEkYckpmUavEvKzCyHE0YBPnnPzOzVnDAKyHqEYWb2Kk4YBST1pDyHYWY2yAmjgGxzIx079xDh8iBmZuCEUVA200hvf7DN5UHMzIAyJgxJiyRtkpT3etySLpS0TdLSdPl0zrpLJD0vaaWkT5YrxmKymQbAJ++ZmQ0q5wjjduCSA/R5OCLOSJebASTVAl8CLgVeC1wj6bVljDOvwZP3NvnkPTMzoIwJIyIeAjqHsek5wMqIWBURPcC3gcsPaXAl2FcexBPfZmZQ+TmM10t6StJPJJ2cts0C1ub0aUvb8pJ0vaRWSa3t7e2HLLC9FWs9wjAzAyqbMJYAcyPidOCfgR+k7crTt+ChShFxW0QsjIiFLS0thyy4SePrqauR5zDMzFIVSxgR0RURO9L79wH1krIkI4o5OV1nA+tHO76aGpcHMTPLVbGEIWmmJKX3z0lj6QCeAI6XNF9SA3A1sLgSMSblQTyHYWYGUFeuJ5Z0F3AhkJXUBtwI1ANExK3AlcDHJPUBu4CrIzlLrk/SDcD9QC2wKCKWlyvOYlxPysxsn7IljIi45gDrbwFuKbDuPuC+csR1MLKZRlZs3F7pMMzMxoRKHyU1pmWbG9i8o8flQczMcMIoqiXTSE//AF27+yodiplZxTlhFLH3XAzPY5iZOWEUk/W1vc3M9nLCKGJfeRAnDDMzJ4wi9las9QjDzMwJo5gpTQ3U1sgn75mZ4YRRVE2NmDrB5UHMzMAJ44B8treZWcIJ4wCymQbavUvKzMwJ40BaMo2e9DYzwwnjgLLNyS4plwcxs2rnhHEA2UwDe/oG2LHH5UHMrLo5YRyAz/Y2M0s4YRzAvnpSnvg2s+rmhHEALg9iZpZwwjgAV6w1M0uULWFIWiRpk6RlBda/V9LT6fKopNNz1q2R9IykpZJayxVjKaZOaKBGridlZlbOEcbtwCVF1q8G3hwRpwGfAW4bsv4tEXFGRCwsU3wlqU3Lg/jkPTOrduW8pvdDkuYVWf9ozsPHgNnlimWkXB7EzKzEEYak95TSNgIfAX6S8ziAn0p6UtL1h/B1hsUJw8ys9F1Snyqx7aBJegtJwvjznObzI+Is4FLg45LeVGT76yW1Smptb28/FCHtJ5txxVozs6K7pCRdCrwdmCXpizmrJgIjPvVZ0mnA14FLI6JjsD0i1qe3myTdC5wDPJTvOSLiNtL5j4ULF5alfkc208jm7Z7DMLPqdqARxnqgFdgNPJmzLAYuHskLSzoG+D7w/oh4Iad9gqTmwfvARUDeI61GS7a5kV29/ex0eRAzq2JFRxgR8RTwlKRvRUQvgKQpwJyI2FJsW0l3ARcCWUltwI1Affq8twKfBqYBX5YE0JceETUDuDdtqwO+FRH/Meyf8BDILQ8yobFsxwmYmY1ppX76PSDpsrT/UqBd0oMR8aeFNoiIa4o9YURcB1yXp30VcPr+W1RO7tne87ITKhyNmVlllDrpPSkiuoB3A9+IiLOB3y5fWGNLNtMA+GxvM6tupSaMOklHAVcBPy5jPGNSy+AuKZ+8Z2ZVrNSEcTNwP/BiRDwhaQGwonxhjS1TJzQglwcxsypX0hxGRHwX+G7O41XAFeUKaqypq61hSpPPxTCz6lbqmd6zJd2bFhPcKOkeSWO2lEc5+OQ9M6t2pe6S+gbJuRdHA7OAH6VtVSMpD+I5DDOrXqUmjJaI+EZE9KXL7UBLGeMac1xPysyqXakJY7Ok90mqTZf3AR0H3OoIkpQHccIws+pVasL4MMkhtRuAV4ArgQ+VK6ixKNvcwM6efrp7XB7EzKpTqQnjM8C1EdESEdNJEshNZYtqDNp7qVYXITSzKlVqwjgtt3ZURHQCZ5YnpLFpsDxIu+cxzKxKlZowatKigwBImkoZr9Y3Fg2e7e2JbzOrVqV+6H8eeFTS90iuhncV8NmyRTUGZZ0wzKzKlXqm952SWoHfAgS8OyKeLWtkY8y0wQKEnsMwsypV8m6lNEFUVZLIVV9bw+Smeo8wzKxqlTqHYfjkPTOrbk4YB8H1pMysmjlhHATXkzKzalbWhCFpUVrhdlmB9ZL0RUkrJT0t6aycdddKWpEu15YzzlK5PIiZVbNyjzBuBy4psv5S4Ph0uR74Cuw9z+NG4FzgHODG3PNAKqWluZHte/rY3dtf6VDMzEZdWRNGRDwEdBbpcjlwZyQeAyanl4K9GHggIjrTM8wfoHjiGRWD1/Zu9yjDzKpQpecwZgFrcx63pW2F2vcj6XpJrZJa29vbyxYo7CsP4olvM6tGlU4YytMWRdr3b4y4LSIWRsTClpbyXqJj39nenvg2s+pT6YTRBszJeTwbWF+kvaJcHsTMqlmlE8Zi4APp0VLnAdsi4hXgfuAiSVPSye6L0raK2lcexAnDzKpPWSvOSroLuBDISmojOfKpHiAibgXuA94OrAS6SS/KFBGdkj4DPJE+1c1pSfWKaqyrZeK4Oo8wzKwqlTVhRMQ1B1gfwMcLrFsELCpHXCORbfbJe2ZWnSq9S+qwk800+iJKZlaVnDAOUosLEJpZlXLCOEjZTIMnvc2sKjlhHKRsppGu3S4PYmbVxwnjIGXTs707dnri28yqixPGQWoZPHnPu6XMrMo4YRykrOtJmVmVcsI4SIMVa50wzKzaOGEcJBcgNLNq5YRxkMbV19LcWOdrYphZ1XHCGIakPIgThplVFyeMYchmGpwwzKzqOGEMQzbjAoRmVn2cMIYh63pSZlaFnDCGIZtpZGt3Lz19A5UOxcxs1DhhDEO2OTkXo2OnRxlmVj2cMIZhX3kQz2OYWfUoa8KQdImk5yWtlPTJPOu/IGlpurwgaWvOuv6cdYvLGefBcnkQM6tGZbtEq6Ra4EvA24A24AlJiyPi2cE+EfEnOf3/GDgz5yl2RcQZ5YpvJAZHGL7ynplVk3KOMM4BVkbEqojoAb4NXF6k/zXAXWWM55DZVx7ECcPMqkc5E8YsYG3O47a0bT+S5gLzgZ/nNI+T1CrpMUnvLPQikq5P+7W2t7cfirgPaHxDLRMaaj2HYWZVpZwJQ3naokDfq4HvRUTuZeyOiYiFwO8D/yjp2HwbRsRtEbEwIha2tLSMLOKD4PIgZlZtypkw2oA5OY9nA+sL9L2aIbujImJ9ersK+AWvnt+oOJ+8Z2bVppwJ4wngeEnzJTWQJIX9jnaSdCIwBfhVTtsUSY3p/SxwPvDs0G0ryfWkzKzalC1hREQfcANwP/Ac8J2IWC7pZkmX5XS9Bvh2ROTurnoN0CrpKeC/gM/lHl01FrielJlVm7IdVgsQEfcB9w1p+/SQxzfl2e5R4NRyxjZS2UwjnTt76O0foL7W5z+a2ZHPn3TDNHjyXudOjzLMrDo4YQzT3pP3fOU9M6sSThjD1JIWIPTEt5lVCyeMYdp3trd3SZlZdXDCGCaXBzGzauOEMUwTGusYX1/LZs9hmFmVcMIYgWyzT94zs+rhhDECPnnPzKqJE8YIuJ6UmVUTJ4wRcMIws2rihDECLZkGOnb20Nc/UOlQzMzKzgljBLLNjURAZ7fnMczsyOeEMQKD5UF85T0zqwZOGCMwWIDQ8xhmVg2cMEbAZ3ubWTVxwhiBbMYFCM2sejhhjECmsY7GuhqfvGdmVaGsCUPSJZKel7RS0ifzrP+gpHZJS9Plupx110pakS7XljPO4ZKUnIvhelJmVgXKdolWSbXAl4C3AW3AE5IW57k2990RccOQbacCNwILgQCeTLfdUq54hyvb3Ei7d0mZWRUo5wjjHGBlRKyKiB7g28DlJW57MfBARHSmSeIB4JIyxTkiLZkG75Iys6pQzoQxC1ib87gtbRvqCklPS/qepDkHuS2SrpfUKqm1vb39UMR9ULKZRl+m1cyqQjkThvK0xZDHPwLmRcRpwM+AOw5i26Qx4raIWBgRC1taWoYd7HBlM4107txD/0De8MzMjhjlTBhtwJycx7OB9bkdIqIjIga/nn8NOLvUbceKbKaBgYAtLg9iZke4ciaMJ4DjJc2X1ABcDSzO7SDpqJyHlwHPpffvBy6SNEXSFOCitG3MaWkeB/hcDDM78pXtKKmI6JN0A8kHfS2wKCKWS7oZaI2IxcD/kHQZ0Ad0Ah9Mt+2U9BmSpANwc0R0livWkdh78t72HphZ4WDMzMqobAkDICLuA+4b0vbpnPufAj5VYNtFwKJyxncouJ6UmVULn+k9Qq4nZWbVoqwjjGowcVwdDXU1/Msjq1m2bhsnzGzmpJnNnDhzIkdPGoeU74AvM7PDjxPGCEni0+94LT97biO/Xt3JD5buO5ireVwdJ85o5sQ0iZwwo5mTZk5kUlN9BSM2MxseRRw55w8sXLgwWltbKxrDtl29vLBxO7/ZsJ3nN3TxwoYd/GZDF127+/b2mTlx3N4kcmKaSF5z1ERqazwaMbPRJenJiFhYSl+PMA6xSePred28qbxu3tS9bRHBhq7d/GbDdl7YsJ3nNyQJ5VerOujpS64HftLMZv763ady1jFTKhW6mVlRHmFUUF//AGs6ulny8ha+8MALbOjazfvOncv/uuREJo7zbiszKz+PMA4TdbU1HDc9w3HTM7z91KP4/E+f545H13D/8g3cdNnJXHrKTE+am9mY4cNqx4hMYx03/u7J/ODj59PS3MgffXMJH7mjlbYt3ZUOzcwMcMIYc06bPZkffvx8/vJ3XsNjqzp42z88xNceWkVf/0ClQzOzKueEMQbV1dZw3RsX8NM/eRNvOHYan73vOS675Zc8tXZrpUMzsyrmhDGGzZ7SxNevXchX3nsWm3fs4V1f/iU3LV7O9t29lQ7NzKqQE8YYJ4lLTz2Kn/3Zm3nfeXO541dreNs/PMT9yzdUOjQzqzJOGIeJiePqufnyU/j+x97A5KZ6/uBfn+Sjd7ayfuuuSodmZlXC52Echnr7B1j0yGq+8LMXqJX4s4tO5B2nHUXvQNDXP0Bv/wC9/ZHe7rvf1x/0pLe9/QOvuj+5qZ5jpjYxd9oEpjTV+3BeK7uIoGt3H5u6djO5qYGWtPKzja6DOQ/DCeMwtrazm7/8wTIefOHQXsu8ubGOOVObmDutiWOmNnFMejt36gSOnjyOutqDG5ju6eunffseNnbtoX37bjZ27WHT3ts9bOrazfSJ47jguGlccFwLJ81spsZlUorq2LGHX6/u5PF02d3bz9xpTcybNoF505qYl53AvGkTmD1l/EH/vkYqItja3cvG7bvZNPg73ns/uR1ctyetdFBXI3739KO57o3zOfnoSaMab1//AH0Dwbj62lF93bHCCaOKRAS/eKGddVt2UV8r6mtr0iW5X5dzv37I/boa0VBXQ22N2LKzh5c6unm5M1le6tjJy53drN2ya2/5EoDaGjFr8njmTmtKksrUJJkEsKlrNxu379nvg2Fr9/6T9LU1oiXTyIyJjbQ0N/JSRzcrNu0AkotSveHYLBccn+WC47IcPXn8IX/f2rfvYdm6bTyzbhvL1m1jWqaBd505m9fNmzImR1ebunbz2OpOHl/VweOrO1mZvlfj6ms4e+4UJo2vZ83mbtZ07KS7p3/vdnU1YvaU8XsTyLxpTczNTmD+tAnMmjKe+iLJJCLY0zdA165eunb3sm1XH127e9PHfXvbu3b10blzMPnvoX37HnryHAbe3FjH9ImNTG8ex/SJjcyYOI7pzcnv/6m12/j2Ey/T3dPPBcdlue6N83nzCS1l/V2s2Lid77Su5d7/XseW7l5Omz2J8xZM4/ULprFw3hSaGqrjvGYnDDtkBgaSOlgvd3bzckc3L3Xu5OXOXbzcsZOXOrv3SwZ1NaKluZHp6YfBjPQDYkbOB8X05nFMm9Cw3yhiY9duHlmxmUdWJkv79uQaIwtaJnDBcUnyOO/YaQddNmUwOTzdti9BbOjaDYAE86dNYEPXbrp7+pkzdTzvPnM2V5w1m2OmNY3gnRuZdVt3JclhVSePr+5gTUdyAmemsY6z507h3AVTOXf+NE6dNYmGun0f+hFB+449vNTRzerNO3mpY+feRLJm80525kkmc6dNINNY96pksD1NBPk++HM11NYwcXw9U5rq9yWAiY3MyPldD/7uxzcU/wa/rbuXb/36Zb7xy9Vs2r6HE2c0c90b53PZGUfTWHdovv137e7lx0+9wnda17J07Vbqa8VbT5rB/JYJ/Hp1J0+t3UrfQFBXI06fM5nzFkzl9QuynD13ygHjP1w5Ydio2barl7Wd3dRITJ/YyNSm/RPBcEQEL2zcwcMr2nlk5WYeX9XJrt5+amvE6bMnccHxLVxwXJYzj5n8qm/JB0wO2QmcOmvS3uXkWZPINNbR3dPHfyzbwD1L2nj0xQ4i4Jx5U7ni7FlceupRZa3tFRG83NnN46s6eWx1kiTWpQczDBazPHf+VM5dMJXXHjVx2LuYIoLNO3r2Jo81HTtZ09HNms072dXbz8Rx9UwcX8/EcXXpbT0Tx9cVbS/HbpyevgEWP7Werz+8it9s2M705kY+eP483nvO3GFdGmBgIHh8dSffbV3LfcteYXfvACfMyHDVwjm868xZTMvsmzvZuaePJ1/awq9WdfCrFzt4Zt02+geC+lpxxpzJe0cgZ82dckh+9p6+Abbt6mXHnj66e/rY1dNPd7rs6u1LbnPbepK27t7B9mSbCY11fOuj5w0rhjGTMCRdAvwTyTW9vx4Rnxuy/k+B60iu6d0OfDgiXkrX9QPPpF1fjojLDvR6ThhHrp6+AZa8vGXvCOTptq0MBExoqOXcBdOokUpODgeyfusu7v3vddyzpI1V7TtprKvh4pNncsXZs7nguOyIytBHBOu27mLZui6WrdvGsvVJUtu8oweAaRMaOGf+VM6Zn4wgqnk+JyJ4eMVmvvbwKh5esZmmhlquWjiHj1wwnzlTDzz6W791F/c82cZ3n2zj5c5umhvruOyMo7lq4RxOmz2ppN1dO/b08cSaTh5b1cFjaQIZiGRkdcYx+xLIgpYJdO3qZVuRpWtX3359dvX2HzCGQQ21NYxvqKWpoXbvbVN9HeMbamlpbuTv33N6yc+Va0wkDEm1wAvA24A24Angmoh4NqfPW4DHI6Jb0seACyPi99J1OyIiczAP8llgAAALrUlEQVSv6YRRPbZ19/KrVUnyeHRlB4hhJYdiIoKla7fy/SXrWPzUerbt6mV6cyPvOnMWV5w9mxNmNB9w+5c7u1m2rotn1m1jeZoctqS78WprxPHTM5wya1Ky+2P+VI6bnhmTcyiV9uz6Lr7+yCoWL13PQASXnnIUH33TAs6YM/lV/fb09fPAsxv5TmsbD69oJwLecOw0rlo4h4tPnjni3Urbd/emCaSTX73YwfL1SQIpJtNYx6TxySht0vjk/tAlM66OpoY6xtfnJoS6fffra8t28MJYSRivB26KiIvTx58CiIj/v0D/M4FbIuL89LETho0Ze/r6+flzm7hnSRu/eL6dvoHg1FmTuOKsWVx2xiwmj69ndcfOZNSwblsygli/je3phbPqa8WJM5s55egkmZ06axInzWyu2iNzhuuVbbu4/dE1fOvxl9m+u49z5k3lujfO5+jJ4/nek238YOk6tnb3cvSkcVy5cA7vOXt2SaOR4dq2q5cnVneyoWv33g//iTmJYOK4ulE/Su1gjZWEcSVwSURclz5+P3BuRNxQoP8twIaI+Kv0cR+wlGR31eci4gcFtrseuB7gmGOOOfull1465D+LWa7NO/aweOl67lnSxvL1XdTViMa6mr0Tyg11NbzmqImccvRETkmTw/EzMods4taSXUV3P7GWRY+s3jvf01Bbw8WnzOSqhbN5w7Ej23VYTcZKwngPcPGQhHFORPxxnr7vA24A3hwRe9K2oyNivaQFwM+Bt0bEi8Ve0yMMG22/2dDFvf+9jt09/XtHDsdNzxQ9XNUOnb7+Ae5fvpFtu3p5+6kzmdzUUOmQDjtj5QJKbcCcnMezgfVDO0n6beAvyEkWABGxPr1dJekXwJlA0YRhNtpOmjmRT106sdJhVK262hp+57SjKh1G1Sjn16AngOMlzZfUAFwNLM7tkM5bfBW4LCI25bRPkdSY3s8C5wPPYmZmFVO2EUZE9Em6Abif5LDaRRGxXNLNQGtELAb+DsgA302PDBk8fPY1wFclDZAktc/lHl1lZmajzyfumZlVsYOZw/DMnJmZlcQJw8zMSuKEYWZmJXHCMDOzkjhhmJlZSY6oo6QktQPDrQ2SBTYfwnAONcc3Mo5vZBzfyIzl+OZGREspHY+ohDESklpLPbSsEhzfyDi+kXF8IzPW4yuVd0mZmVlJnDDMzKwkThj73FbpAA7A8Y2M4xsZxzcyYz2+kngOw8zMSuIRhpmZlcQJw8zMSlJ1CUPSJZKel7RS0ifzrG+UdHe6/nFJ80YxtjmS/kvSc5KWS/qfefpcKGmbpKXp8unRii99/TWSnklfe7/SwEp8MX3/npZ01ijGdmLO+7JUUpekTwzpM6rvn6RFkjZJWpbTNlXSA5JWpLdTCmx7bdpnhaRrRzG+v5P0m/T3d6+kyQW2Lfq3UMb4bpK0Lud3+PYC2xb9Xy9jfHfnxLZG0tIC25b9/TvkIqJqFpLrcrwILAAagKeA1w7p80fAren9q4G7RzG+o4Cz0vvNwAt54rsQ+HEF38M1QLbI+rcDPwEEnAc8XsHf9QaSk5Iq9v4BbwLOApbltP0t8Mn0/ieBv8mz3VRgVXo7Jb0/ZZTiuwioS+//Tb74SvlbKGN8NwH/Twm//6L/6+WKb8j6zwOfrtT7d6iXahthnAOsjIhVEdEDfBu4fEify4E70vvfA96q9OpO5RYRr0TEkvT+duA5YNZovPYhdDlwZyQeAyZLqsQ1NN8KvBgRwz3z/5CIiIeAziHNuX9jdwDvzLPpxcADEdEZEVuAB4BLRiO+iPhpRPSlDx8jubxyRRR4/0pRyv/6iBWLL/3cuAq461C/bqVUW8KYBazNedzG/h/Ie/uk/zTbgGmjEl2OdFfYmcDjeVa/XtJTkn4i6eRRDQwC+KmkJyVdn2d9Ke/xaLiawv+olXz/AGZExCuQfEkApufpM1bexw+TjBjzOdDfQjndkO4yW1Rgl95YeP/eCGyMiBUF1lfy/RuWaksY+UYKQ48rLqVPWUnKAPcAn4iIriGrl5DsZjkd+GfgB6MZG3B+RJwFXAp8XNKbhqwfC+9fA3AZ8N08qyv9/pVqLLyPfwH0Ad8s0OVAfwvl8hXgWOAM4BWS3T5DVfz9A66h+OiiUu/fsFVbwmgD5uQ8ng2sL9RHUh0wieENiYdFUj1JsvhmRHx/6PqI6IqIHen9+4B6SdnRii8i1qe3m4B7SYb+uUp5j8vtUmBJRGwcuqLS719q4+BuuvR2U54+FX0f00n2dwDvjXSH+1Al/C2URURsjIj+iBgAvlbgdSv9/tUB7wbuLtSnUu/fSFRbwngCOF7S/PRb6NXA4iF9FgODR6RcCfy80D/MoZbu8/wX4LmI+IcCfWYOzqlIOofkd9gxSvFNkNQ8eJ9kcnTZkG6LgQ+kR0udB2wb3P0yigp+s6vk+5cj92/sWuCHefrcD1wkaUq6y+WitK3sJF0C/DlwWUR0F+hTyt9CueLLnRN7V4HXLeV/vZx+G/hNRLTlW1nJ929EKj3rPtoLyVE8L5AcQfEXadvNJP8cAONIdmWsBH4NLBjF2C4gGTY/DSxNl7cDfwj8YdrnBmA5yVEfjwFvGMX4FqSv+1Qaw+D7lxufgC+l7+8zwMJR/v02kSSASTltFXv/SBLXK0Avybfej5DMif0nsCK9nZr2XQh8PWfbD6d/hyuBD41ifCtJ9v8P/g0OHjV4NHBfsb+FUYrvX9O/radJksBRQ+NLH+/3vz4a8aXttw/+zeX0HfX371AvLg1iZmYlqbZdUmZmNkxOGGZmVhInDDMzK4kThpmZlcQJw8zMSuKEYVaitNLtj0ew/TvLVR1X0mclrZW0Y0h73urLkk6VdHs5YrEjlxOG2ej5f4Evj/RJJNXmaf4R+c8U/giwJSKOA75AUn2WiHgGmC3pmJHGY9XDCcOOKJLeJ+nX6TUGvjr44Spph6TPS1oi6T8ltaTtZ0h6LOfaD1PS9uMk/SwtUrhE0rHpS2QkfU/J9SK+mXPW+OckPZs+z9/niesEYE9EbE4f3y7pVkkPS3pB0jvS9lol16N4In2uP0jbL1RyrZRvkZy09ioR8VjkP6O+WPXlH5GcAW1WEicMO2JIeg3weyRF3c4A+oH3pqsnkNSXOgt4ELgxbb8T+POIOI3kg3iw/ZvAlyIpUvgGkrN5Iakg/AngtSRn654vaSpJiYqT0+f5qzzhnU9S+DDXPODNwO8At0oaRzIi2BYRrwNeB3xU0vy0/zkkZwS/9iDelmLVl1tJKqqalaSu0gGYHUJvBc4Gnki/RI9nX2G/AfYVgvs34PuSJgGTI+LBtP0O4LtpjZ9ZEXEvQETsBkif89eR1gdSciW1eSQlRnYDX5f070C+eY6jgPYhbd+JpIDeCkmrgJNIagqdJunKtM8k4HigJ33t1Qf5nhSr2rqJpFyFWUmcMOxIIuCOiPhUCX2L1cQpdsGsPTn3+0muTNeXFjJ8K8kunhuA3xqy3S6SD/9iMUT62n8cEa8qNCjpQmBnkbgKGaza2pan+vK4NC6zkniXlB1J/hO4UtJ02Hvt7LnpuhqS6sMAvw88EhHbgC2SBnfLvB94MJJrkLRJemf6PI2Smgq9qJLrl0yKpFz6J0iu0zDUc8BxQ9reI6kmnR9ZADxPUpH2Y2mZeySdkFYzHa5i1ZdP4HCokGpjhkcYdsSIiGcl/SXJVcxqSCqIfhx4ieTb+cmSniTZj/976WbXkswfNJFcN/tDafv7ga9Kujl9nvcUeelm4IfpHISAP8nT5yHg85KU84H9PMl8ygySyqa7JX2dZDfXknRyup38l3B9FUl/S5IImyS1kVS9vYmkXP6/SlpJMrLIneR+C/DvB3pus0GuVmtVQdKOiMhUOIZ/An4UET9Lz4H4cUR8r0KxNJIkqwti3/W7zYryLimz0fPXJNfrGAuOAT7pZGEHwyMMMzMriUcYZmZWEicMMzMriROGmZmVxAnDzMxK4oRhZmYl+b8bEk6e8LpaTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Training Accuarcy: \n",
      "Accuracy: 0.91174\n",
      "Test Accuarcy: \n",
      "Accuracy: 0.8685\n",
      "\n",
      "Cost after epoch 0: 1.593570\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-abd280ed1d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# train 3-layer model - Minibatch SGD with Momentum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"momentum\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00007\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"No\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-a0fb4d0a7e8a>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, layers_dims, optimizer, regularizer, keep_prob, lambd, learning_rate_init, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters_with_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"momentum\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters_with_momentum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# Adam counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-ba8b0a1e49a0>\u001b[0m in \u001b[0;36mupdate_parameters_with_momentum\u001b[1;34m(parameters, grads, v, beta, learning_rate)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'db'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train 3-layer model - Adam \n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,learning_rate_init = 0.0007,regularizer = \"No\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Adam\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - Minibatch SGD with Momentum \n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\",num_epochs = 200,mini_batch_size = 1,learning_rate_init = 0.00007,regularizer = \"No\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"Minibatch GD with Momentum\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      \n",
    "# train 3-layer model - GD \n",
    "layers_dims = [train_X.shape[0], 25, 12, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\",num_epochs = 200,mini_batch_size = 50000,learning_rate_init = 1,regularizer = \"No\",keep_prob=0.7, lambd=0.7)\n",
    "\n",
    "# Predict\n",
    "print(\"GD\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
