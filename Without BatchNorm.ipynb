{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import h5py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = h5py.File('train_128.h5', 'r')\n",
    "list(f1.keys())\n",
    "X1 = f1['data']\n",
    "X_full= np.array(X1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = h5py.File('train_label.h5', 'r')\n",
    "list(f1.keys())\n",
    "Y1 = f1['label']\n",
    "Y_full= np.array(Y1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    #Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "def normalize(X):\n",
    "    mu_train = np.mean(X,axis=0,keepdims=True)\n",
    "    var_train = np.var(X,axis=0,keepdims=True)\n",
    "    X_norm = (X-mu_train) / np.sqrt(var_train)\n",
    "    \n",
    "    return X_norm, mu_train, var_train\n",
    "\n",
    "def normalize_test(X_norm, mu_train, var_train):\n",
    "    X_norm = (X_norm-mu_train) / np.sqrt(var_train)\n",
    "    \n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50000)\n",
      "(10, 50000)\n",
      "(128, 10000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_X = X_full[0:50000]\n",
    "train_X, mu_train, var_train = normalize(train_X)\n",
    "train_X = train_X.T\n",
    "\n",
    "train_Y = Y_full[0:50000]\n",
    "train_Y = oneHotIt(train_Y).T\n",
    "\n",
    "dev_X = X_full[50000:]\n",
    "dev_X = normalize_test(dev_X,mu_train,var_train)\n",
    "dev_X = dev_X.T\n",
    "\n",
    "dev_Y = Y_full[50000:]\n",
    "dev_Y = oneHotIt(dev_Y).T\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(dev_X.shape)\n",
    "print(dev_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = a3.T\n",
    "    labels = Y.T\n",
    "    \n",
    "    cost = -np.sum((np.log(np.sum(labels*logits,axis=1, keepdims=True))))/logits.shape[0]\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = softmax(z3.T).T\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = a3 - Y\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((10,m), dtype = np.int)\n",
    "    accuracy = 0 \n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for j in range(0,10):\n",
    "        for i in range(0, a3.shape[1]):\n",
    "            if a3[j,i] == np.max(a3[:,i]):\n",
    "                p[j,i] = 1\n",
    "            else:\n",
    "                p[j,i] = 0\n",
    "    \n",
    "    for i in range(0,a3.shape[1]):\n",
    "        accuracy += (p[:,i] == y[:,i]).all()\n",
    "    accuracy = accuracy / a3.shape[1]\n",
    "    print(\"Accuracy: \"  + str(accuracy))\n",
    "    \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + W3*lambd/m\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + W2*lambd/m\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + W1*lambd/m\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (20, 2)\n",
    "                    b1 -- bias vector of shape (20, 1)\n",
    "                    W2 -- weight matrix of shape (3, 20)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
    "    cache -- tuple, information stored for computing the backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> RELU -> LINEAR -> Batch-Norm -> SOFTMAX\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    D1 = np.random.rand(A1.shape[0],A1.shape[1])      # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob)                             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A1 = A1*D1                                        # Step 3: shut down some neurons of A1\n",
    "    A1 = A1/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    D2 = np.random.rand(A2.shape[0],A2.shape[1])       # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    D2 = (D2 < keep_prob)                              # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A2 = A2*D2                                         # Step 3: shut down some neurons of A2\n",
    "    A2 = A2/keep_prob                                  # Step 4: scale the value of neurons that haven't been shut down\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = softmax(Z3.T).T\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout_L2(X, Y, cache, keep_prob, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + W3*lambd/m\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    dA2 = dA2*D2                                    # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2/keep_prob                             # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + W2*lambd/m\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "\n",
    "    dA1 = dA1*D1                                   # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1/keep_prob                            # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + W1*lambd/m\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    dA2 = dA2*D2                                     # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2/keep_prob                              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1 = dA1*D1                                     # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1/keep_prob                              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_mean_var(x, eps):\n",
    "\n",
    "  N, D = x.shape\n",
    "\n",
    "  #step1: calculate mean\n",
    "  mu = 1./N * np.sum(x, axis = 0)\n",
    "\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "  xmu = x - mu\n",
    "\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "  sq = xmu ** 2\n",
    "\n",
    "  #step4: calculate variance\n",
    "  var = 1./N * np.sum(sq, axis = 0)\n",
    "\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "  sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "  #step6: invert sqrtwar\n",
    "  ivar = 1./sqrtvar\n",
    "\n",
    "  return mu,ivar,sqrtvar,var,eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W' + str(l+1)] - learning_rate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b' + str(l+1)] - learning_rate*grads['db' + str(l+1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        \n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "   \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- python dictionary containing your updated velocities\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "    \n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)]+(1-beta)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)]+(1-beta)*grads['db' + str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v[\"db\" + str(l+1)]\n",
    "     \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads[\"db\" + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**(t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**(t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*(grads[\"dW\" + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*(grads[\"db\" + str(l+1)]**2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**(t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**(t))\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v_corrected[\"dW\" + str(l+1)]/((s_corrected[\"dW\" + str(l+1)])**(1/2)+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v_corrected[\"db\" + str(l+1)]/((s_corrected[\"db\" + str(l+1)])**(1/2)+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, regularizer = \"No\",keep_prob = 0.85, lambd = 0.7, learning_rate_init = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        learning_rate = learning_rate_init \n",
    "        #learning_rate = learning_rate_init * (1/(1+0.01*i))\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            if regularizer == \"DropOut\":\n",
    "                a3, caches = forward_propagation_with_dropout(minibatch_X, parameters,keep_prob)\n",
    "            elif regularizer == \"DropOut_L2\":\n",
    "                a3, caches = forward_propagation_with_dropout(minibatch_X, parameters,keep_prob)\n",
    "            else:\n",
    "                a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            if regularizer == \"No\":\n",
    "                grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "            elif regularizer == \"L2\":\n",
    "                grads = backward_propagation_with_regularization(minibatch_X, minibatch_Y, caches,lambd)\n",
    "            elif regularizer == \"DropOut\":\n",
    "                grads = backward_propagation_with_dropout(minibatch_X, minibatch_Y, caches, keep_prob)\n",
    "            elif regularizer == \"DropOut_L2\":\n",
    "                grads = backward_propagation_with_dropout_L2(minibatch_X, minibatch_Y, caches, keep_prob, lambd)\n",
    "                \n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 10)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.367857\n",
      "Cost after epoch 10: 0.719520\n",
      "Cost after epoch 20: 0.557748\n",
      "Cost after epoch 30: 0.438340\n",
      "Cost after epoch 40: 0.373082\n",
      "Cost after epoch 50: 0.352060\n",
      "Cost after epoch 60: 0.367889\n",
      "Cost after epoch 70: 0.321277\n",
      "Cost after epoch 80: 0.320698\n",
      "Cost after epoch 90: 0.270974\n",
      "Cost after epoch 100: 0.296253\n",
      "Cost after epoch 110: 0.336461\n",
      "Cost after epoch 120: 0.332615\n",
      "Cost after epoch 130: 0.329769\n",
      "Cost after epoch 140: 0.327614\n",
      "Cost after epoch 150: 0.253603\n",
      "Cost after epoch 160: 0.318444\n",
      "Cost after epoch 170: 0.271955\n",
      "Cost after epoch 180: 0.287924\n",
      "Cost after epoch 190: 0.280376\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam + Dropout with Weight Decay\n",
      "Training Accuarcy: \n",
      "Accuracy: 0.92966\n",
      "Test Accuarcy: \n",
      "Accuracy: 0.8896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train 3-layer model - Adam + Dropout_L2\n",
    "layers_dims = [train_X.shape[0], 200, 100, 10]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\",num_epochs = 200,mini_batch_size = 500,learning_rate_init = 0.0001,regularizer = \"DropOut_L2\",keep_prob=0.8, lambd=1.75)\n",
    "\n",
    "# Predict\n",
    "print(\"Adam + Dropout with Weight Decay\")\n",
    "print(\"Training Accuarcy: \")\n",
    "predict(train_X, train_Y, parameters)\n",
    "print(\"Test Accuarcy: \")\n",
    "predict(dev_X, dev_Y, parameters)\n",
    "print()\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
